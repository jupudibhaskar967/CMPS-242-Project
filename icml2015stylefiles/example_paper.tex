%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2015 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2015,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}
\usepackage{pdflscape}
\usepackage{tabularx}
\usepackage{lipsum}
\usepackage{amsmath,array}
\usepackage{kantlipsum}
\usepackage{float}
% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2015} with
% \usepackage[nohyperref]{icml2015} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage[accepted]{icml2015} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
\usepackage[accepted]{icml2015}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Sentimental analysis for yelp dataset}

\begin{document} 





\twocolumn[
\icmltitle{Sentimental analysis for yelp dataset}


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2015
% package.
\icmlauthor{Bhaskar Jupudi}{njupudi@ucsc.edu}
\icmlauthor{Trivikram Bollempalli}{tbollemp@ucsc.edu}
\icmlauthor{Chandrahas}{cjagadis@ucsc.edu}
\icmlauthor{Karthikeyan}{karthik@ucsc.edu}
\icmlauthor{GitHub Link}{https://github.com/jupudibhaskar967/CMPS-242-Project.git}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}
\vskip 0.3in
]




\begin{abstract} 
In this project, we aim to perform sentiment analysis i.e., classifying whether the review is postive or negative using the yelp dataset based on reviews and ratings. The classification problem can be solved by a set of algorithms. Every algorithm has its own advantages and disadvantages in terms of accuracy and model complexity. 

For example, Naive Bayes classifier is faster to compute than Logistic Regression classifier for huge datasets. But the disadvantage with the former is that it assumes that features are independent where as the latter has no such assumptions which can lead to better precdiction. Our work mainly concentrates on implementing these two classifiers and techniques to make them perform much better. We have adopted multi-processing for feature extraction to make it way faster and also implemented two different approaches of Logistic regression for both binary and multi-class classfication. We have also implemented Naive Bayes classifier. Finally, we contrast these two algorithms based on time taken for execution and performance metrics like accuracy, precision and recall. 
\end{abstract} 

\section{Problem statement}


\section{Feature Extraction}

We have extracted the features for respective algorithms using our own constraints and formulations instead of using count vectorizer. The reason we designed this because countvectorizer method is raising a run-time exception for a large dataset (\textgreater300k reviews).

\subsection{Binary classification}
For logistic regression, we need to form a data vector from a bag of words which contains the word count. To compute this, first we extracted 4/5th of our total dataset (which is our training data) and extracted the words and their respective counts into a dictionary. Out of all these words, we have used three constraints to restrict the number of bag of words. One, if a word occurs for 'x' times in a positive review then we considered that word into bag of words only if the same word occurs for less than 4/10th times in the negative review and vice-versa. Second, total word count for a specific word should be more than 14. Third, we check if length of the word is more than 2 characters thereby restricting the number of features to 8895. The exact numbers we have used is just an arbitrary choice and can vary to adjust the accuracy. The intuition behind this is, a word which have equal count over both postive and negative reviews and if a word occurs over all the reviews for very less frequent times (\textless10 or so which might be a typing mistake) will be having a very less impact over the decision rule and single character words may not effect the meaning of the review when considered as a whole.

After obtaining bag of words, we formed data vector 'x', using multi-processing (as it is both compute and memory intensive operation) to make it more faster as each review can be formed into a count vector indepedently. For this we have used multiprocessing.Pool library in python. We used pool.map\_async() method to make all the individual vectors into a single data vector. 

For Naive Bayes algorithm, we directly used all the words in the initial dictionary without using any constraints as there is no requirement for us to form the data vector as we did in logistic regression and the computation is way faster because of the assumption that features are independent of each other. 


\subsection{Multiclass classification}


We did the same thing for Naive Bayes algorithm, as we did in the binary classification.



\section{Model Formulation}
From Feature Engineering, we get input data and its labels. we will use this to train our logistic regression algorithm. We have implemented both logistic regression for two classes and also multi-class logistic regression.
\subsection{Binary Logistic Regression}
The probability of a class in logistic regression with two classes is given by:



We estimated the parameters using Maximum Log Likelihood estimation. For N data points, the likelihood function is : 


Taking log on both sides and negation of it, we get the below negative log likelihood function:


E(w) is our cost function which we want to minimize to estimate W parameters. The gradient for the cost function is given by:



\subsection{Multiclass Logistic regression}
The probability of a class in logistic regression with multiple classes is given by:
p (Ck |x) = yk (φ) = exp (ak ) P j exp (aj) where ak = ln p (x|Ck ) + ln p (Ck )
We estimated the parameters using Maximum Log Likelihood estimation. For N data points, the likelihood function is : 
= Y N n=1 Y K k=1 y tnk nk
Taking log on both sides and negation of it, we get the below negative log likelihood function:
X N n=1 X K k=1 tnk ln ynk
E(w) is our cost function which we want to minimize to estimate W parameters. The gradient for the cost function is given by:
∇E (w1, . . . , wK ) = X N n=1 (ynj − tnj) φn
We use this gradient in minimizing the cost function.

\subsection{Optimization techniques}


\section{Evaluations}

We performed all our experiments on a server that has 24 physical cores (with hyperthreading 2) and 128GB of DRAM. 

\section{Results}

\subsection{Effect of parallelism}
\begin{table}[H]
\caption{Execution time for extraction of features in Logistic regression classifier.}
\label{sample-table}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lllll}
\hline
Parallelism & Size & Features & Time \\
\hline
No  & 100k & 9049 & 65m36.271s \\
No  & 50k & 5323 & 18m32.441s \\
Yes  & 100k & 9049 & 7m8.291s \\
Yes  & 50k & 5323 & 2m36.947s \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}




\begin{table}[H]
\caption{Parallelism vs countvectorizer()}
\label{sample-table}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lllll}
\hline
Method & Features & Time \\
\hline
Parallel  & 17083 & 42m27.394s \\
countvectorizer  & 10k & Run-Time error \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}



\begin{table}[H]
\caption{Performance analysis of LR and NB for binary classification}
\label{sample-table}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llllll}
\hline
Classifier & Features & Time & Accuracy & Precision & Recall\\
\hline
LR with gradient & 9049 & 102m39.110s & 84.81 & FILL & FILL \\
LR with SGD & 13084 & 13m30.578s & 86.87 & [89.41, 80.76] & [91.75, 76.11] \\
Naive Bayes & 199118 & 1m34.385s & 82.98 & [86.80, 73.96] & [88.73, 70.34] \\
\hline
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\end{table}


\begin{table}[H]
\caption{Performance analysis of LR and NB for multiclass classification}
\label{sample-table}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llllll}
\hline
Classifier & Features & Time & Accuracy & Precision & Recall\\
\hline
LR with gradient & -- & 102m39.110s & 84.81 & FILL & FILL \\
LR with SGD & -- & 13m30.578s & 86.87 & [89.41, 80.76] & [91.75, 76.11] \\
Naive Bayes & 219383 & 3m29.505s & 60.60 & [58.5, 33.9, 34.2, 44.6, 75.6] & [77.8, 10.1, 17.2, 55.3, 74.3] \\
\hline
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\end{table}






% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2015}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
\grid
